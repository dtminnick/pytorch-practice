{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwzUGm7V8o1JXTP8Dlm+4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtminnick/pytorch-practice/blob/main/assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook completes the requirements for Deep Learning Module 2"
      ],
      "metadata": {
        "id": "aKw3CV0YiZbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries."
      ],
      "metadata": {
        "id": "UHuxQpk3imHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision.utils import make_grid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "WpEw0TDNiqLO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the MNIST data.\n",
        "\n",
        "Notes:\n",
        "\n",
        "**transforms.ToTensor()** converts the PIL image to a PyTorch tensor and scales pixel values from [0, 255] to [0.0, 1.0].\n",
        "\n",
        "**transforms.Normalize((0.5,), (0.5,))** then normalizes the tensor to have values in the range [-1.0, 1.0], assuming the original data is centered around 0.5.\n",
        "\n",
        "Normalization is applied during data loading."
      ],
      "metadata": {
        "id": "r5fQL4Evj2oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Get training data.\n",
        "\n",
        "training_data = datasets.MNIST(root = \"./data\",\n",
        "                            train = True,\n",
        "                            download = True,\n",
        "                            transform = transform)\n",
        "\n",
        "# Get testing data.\n",
        "\n",
        "testing_data = datasets.MNIST(root = \"./data\",\n",
        "                           train = False,\n",
        "                           download = True,\n",
        "                           transform = transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfwFYgcTj7sA",
        "outputId": "126e4792-d536-42dd-d707-8814da8d10ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.08MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.52MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.74MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm download of training data."
      ],
      "metadata": {
        "id": "tVmkZpxMlSdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set size:\", len(training_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCf5-rxWlW2p",
        "outputId": "9f0f9e30-2e08-44ca-b4de-2ece3b984810"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm download of test data."
      ],
      "metadata": {
        "id": "YkpHtO71lZZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing set size:\", len(testing_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79eecbe-7e94-467d-fa02-ad97744ba927",
        "id": "x6SYUVlc--GF"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect a small image sample from the training set to confirm normalization was applied properly.\n",
        "\n",
        "Correct normalization will show pixel values roughly in the range -1.0 to 1.0 and a mean that hovers around 0.0, since I normalized at 0.5."
      ],
      "metadata": {
        "id": "xDOHSh1ypl2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    img_tensor, label = training_data[i]\n",
        "    print(f\"Image {i} - Label: {label}\")\n",
        "    print(f\"  Min pixel value: {img_tensor.min().item():.4f}\")\n",
        "    print(f\"  Max pixel value: {img_tensor.max().item():.4f}\")\n",
        "    print(f\"  Mean pixel value: {img_tensor.mean().item():.4f}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "952vbGRmpr0l",
        "outputId": "180b258c-db99-431c-a47f-d34792db6b50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 0 - Label: 5\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.7246\n",
            "----------------------------------------\n",
            "Image 1 - Label: 0\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.6889\n",
            "----------------------------------------\n",
            "Image 2 - Label: 4\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.8055\n",
            "----------------------------------------\n",
            "Image 3 - Label: 1\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.8286\n",
            "----------------------------------------\n",
            "Image 4 - Label: 9\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.7678\n",
            "----------------------------------------\n",
            "Image 5 - Label: 2\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.7039\n",
            "----------------------------------------\n",
            "Image 6 - Label: 1\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.8235\n",
            "----------------------------------------\n",
            "Image 7 - Label: 3\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.6412\n",
            "----------------------------------------\n",
            "Image 8 - Label: 1\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.8912\n",
            "----------------------------------------\n",
            "Image 9 - Label: 4\n",
            "  Min pixel value: -1.0000\n",
            "  Max pixel value: 1.0000\n",
            "  Mean pixel value: -0.7809\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the sample images alongside their pixel statistics."
      ],
      "metadata": {
        "id": "n4n6Xm1G7hJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 5\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(num_samples):\n",
        "    img_tensor, label = training_data[i]\n",
        "\n",
        "    # Convert tensor to PIL image for display\n",
        "    img = to_pil_image(img_tensor)\n",
        "\n",
        "    # Plot image\n",
        "    plt.subplot(1, num_samples, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Label: {label}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Print pixel stats\n",
        "    print(f\"Image {i}\")\n",
        "    print(f\"  Label: {label}\")\n",
        "    print(f\"  Min: {img_tensor.min().item():.4f}\")\n",
        "    print(f\"  Max: {img_tensor.max().item():.4f}\")\n",
        "    print(f\"  Mean: {img_tensor.mean().item():.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "0UeOFAsj7nxM",
        "outputId": "076d85e7-71ae-43c5-9544-2d17ce0db0dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 0\n",
            "  Label: 5\n",
            "  Min: -1.0000\n",
            "  Max: 1.0000\n",
            "  Mean: -0.7246\n",
            "------------------------------\n",
            "Image 1\n",
            "  Label: 0\n",
            "  Min: -1.0000\n",
            "  Max: 1.0000\n",
            "  Mean: -0.6889\n",
            "------------------------------\n",
            "Image 2\n",
            "  Label: 4\n",
            "  Min: -1.0000\n",
            "  Max: 1.0000\n",
            "  Mean: -0.8055\n",
            "------------------------------\n",
            "Image 3\n",
            "  Label: 1\n",
            "  Min: -1.0000\n",
            "  Max: 1.0000\n",
            "  Mean: -0.8286\n",
            "------------------------------\n",
            "Image 4\n",
            "  Label: 9\n",
            "  Min: -1.0000\n",
            "  Max: 1.0000\n",
            "  Mean: -0.7678\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training, validation and testing sets.\n",
        "\n",
        "A testing set already exists; randomly take 10,000 images from the training set and reserve them as a validation set."
      ],
      "metadata": {
        "id": "hfvRWiqCo1yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 50000\n",
        "val_size = 10000\n",
        "\n",
        "training_data, validation_data = random_split(training_data, [train_size, val_size])"
      ],
      "metadata": {
        "id": "78ZZyILP8j8I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm splits."
      ],
      "metadata": {
        "id": "ejkihwCZ84Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set size:\", len(training_data))\n",
        "print(\"Validation set size:\", len(validation_data))\n",
        "print(\"Testing set size:\", len(testing_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvw791Sd88Bw",
        "outputId": "e687a9cf-0d4c-4aa7-fa8b-e8e6100b75eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 50000\n",
            "Validation set size: 10000\n",
            "Testing set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a CNN network with convolution layers, pooling layers to classify the number."
      ],
      "metadata": {
        "id": "tw4BJSKL9jQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by setting a batch size.\n",
        "\n",
        "Notes on batch size:\n",
        "\n",
        "Batch size determines how many samples from the dataset are processed together in one forward/backward pass during training.\n",
        "\n",
        "**Gradient Estimation**\n",
        "\n",
        "A small batch gives a noisy but fast estimate of the gradient.  And a large batch gives a smoother, more stable gradient but requires more memory to compute.\n",
        "\n",
        "**Training Speed**\n",
        "\n",
        "Smaller batches may converge faster in early epochs but can be less stable.  Larger batches can leverage parallelism, speeding up training.\n",
        "\n",
        "**Generalization**\n",
        "\n",
        "Smaller batches often introduce more stochasticity, which can help escape local minima and improve generalization.  Larger batches may overfit if not regularized properly.\n",
        "\n",
        "Start with a batch size of ten and then monitor loss curve stability, validation accuracy and training time per epoch.  If seeing erratic loss or poor generalization, reduce batch size.  If training is slow and memory allows, increase it."
      ],
      "metadata": {
        "id": "rRNuYNi5_Zzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
        "testing_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "TeuRLS0S_F9u"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define two convolutional layers.\n",
        "\n",
        "One layer that transforms a single-channel image into six feature maps, each capturing different spatial patterns.\n",
        "\n",
        "Then a second layer that takes the six feature maps from the first layer and extracts deeper, more abstract features into 16 maps.\n",
        "\n",
        "Note on kernel size and stride parameters:\n",
        "\n",
        "**Kernel Size**\n",
        "\n",
        "The kernel is a small matrix that slides over the input image to extract features.  Kernel size defines the dimensions of this matrix, typically 3x3 or 5x5.  It controls the receptive field, i.e. how much of the input the filter sees at once.\n",
        "\n",
        "**Stride**\n",
        "\n",
        "Stride controls how far the kernel moves with each step.  A stride of 1 means the kernel moves one pixel at a time.  A stride of 2 skips every other pixel, reducing output size and increasing efficiency.\n",
        "\n",
        "Varying kernel size and stride can influence the CNN’s loss and accuracy. These parameters shape how the model sees and processes spatial patterns, which directly affects feature extraction and learning dynamics.\n",
        "\n",
        "Start with a kernel size of 3 and stride of 1."
      ],
      "metadata": {
        "id": "DK0zbPN7BaFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = nn.Conv2d(in_channels = 1,\n",
        "                  out_channels = 6,\n",
        "                  kernel_size = 3,\n",
        "                  stride = 1)\n",
        "\n",
        "conv2 = nn.Conv2d(in_channels = 6,\n",
        "                  out_channels = 16,\n",
        "                  kernel_size=3,\n",
        "                  stride = 1)"
      ],
      "metadata": {
        "id": "Y3Q8CINHBcgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the training loss and validation loss as a function of epochs."
      ],
      "metadata": {
        "id": "KCdoHPeh9mdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the both training accuracy and validation accuracy as a function of epochs."
      ],
      "metadata": {
        "id": "RtB0CjgH9q74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the testing accuracy."
      ],
      "metadata": {
        "id": "5cegijD19vim"
      }
    }
  ]
}