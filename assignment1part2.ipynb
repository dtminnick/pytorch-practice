{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZQTn/NyV13qRq+WpZPH8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtminnick/pytorch-practice/blob/main/assignment1part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning - Manually Graded Assignment 1 - Part 2 (CIFAR Dataset)**\n",
        "\n",
        "Donnie Minnick\n",
        "\n",
        "October 2025"
      ],
      "metadata": {
        "id": "TZCVLAIIwVlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "ZQxh3j8UwpvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision.utils import make_grid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "cLTm1HJgwskp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load CIFAR Data"
      ],
      "metadata": {
        "id": "imq7zgUIwyN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Get training data.\n",
        "\n",
        "training_data = torchvision.datasets.CIFAR10(root = './data',\n",
        "                                             train = True,\n",
        "                                             download = True,\n",
        "                                             transform = transform)\n",
        "\n",
        "# Get testing data.\n",
        "\n",
        "testing_data = torchvision.datasets.CIFAR10(root = './data',\n",
        "                                            train = False,\n",
        "                                            download = True,\n",
        "                                            transform = transform)\n"
      ],
      "metadata": {
        "id": "QTTuNArIw37e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm download of training data."
      ],
      "metadata": {
        "id": "AAk5VDeJx7Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set size:\", len(training_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjMr0MHGx9M8",
        "outputId": "ba6b6ee6-7776-49d3-f166-9293dcd6981b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm download of testing data."
      ],
      "metadata": {
        "id": "tBt3KVR9yABz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing set size:\", len(testing_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkFeJ2hdyCOS",
        "outputId": "63737ae2-5823-4cfc-cb0f-f2c8834e4ba7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data\n",
        "\n",
        "Split the data into training, validation and testing sets.\n",
        "\n",
        "A testing set already exists; randomly take 10,000 images from the training set and reserve them as a validation set."
      ],
      "metadata": {
        "id": "XjeXe4deyT7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 40000\n",
        "val_size = 10000\n",
        "\n",
        "training_data, validation_data = random_split(training_data, [train_size, val_size])"
      ],
      "metadata": {
        "id": "u4CuVzycyZVU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm splits."
      ],
      "metadata": {
        "id": "8MDt9kdjymWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set size:\", len(training_data))\n",
        "print(\"Validation set size:\", len(validation_data))\n",
        "print(\"Testing set size:\", len(testing_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvhAIvH6ynva",
        "outputId": "2f97b34f-266a-4b50-b986-4572d878d1d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 40000\n",
            "Validation set size: 10000\n",
            "Testing set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a CNN Network\n",
        "\n",
        "Build a CNN network with convolution layers to classify the images."
      ],
      "metadata": {
        "id": "kWEGYGufysJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Data Loaders\n",
        "\n",
        "Batch training and validation data with batch_size = 64. Shuffle training data for generalization; validation data is not shuffled, ensuring consistent evaluation."
      ],
      "metadata": {
        "id": "qI3is047y44C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "training_loader = DataLoader(training_data, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "validation_loader = DataLoader(validation_data, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "testing_loader = DataLoader(testing_data, batch_size = batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "SIOpRdWFy8Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Sweep Strategy\n",
        "\n",
        "Implement a sweep strategy to find the optimal hyperparameters to maximize accuracy.\n",
        "\n",
        "Systematically vary key hyperparameters - kernel size, stride, batch size, and learning rate - to assess their impact on training loss, validation accuracy, and feature map evolution.  This modular sweep enables principled experimentation, helping identify optimal architecture and training settings for improved model performance and stakeholder clarity."
      ],
      "metadata": {
        "id": "lry-xKlN7acH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Sweep Grid\n",
        "\n",
        "Create a sweep grid to explore four key hyperparameters that influence CNN performance."
      ],
      "metadata": {
        "id": "0ssq470H8jWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_grid = {'kernel_size': [3, 5],\n",
        "              'stride': [1, 2],\n",
        "              'batch_size': [32, 64],\n",
        "              'learning_rate': [0.001, 0.01]}"
      ],
      "metadata": {
        "id": "RJllPZZM8Uqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel size** [3, 5] tests the impact of local versus broader spatial feature extraction.\n",
        "\n",
        "**Stride** [1, 2] assesses how spatial resolution and downsizing effect learning.\n",
        "\n",
        "**Batch size** [32, 64] evaluates gradient stability, generalization, and training efficiency.\n",
        "\n",
        "**Learning rate** [0.001, 0.01] measures convergence speed and sensitivity to weight updates.\n",
        "\n",
        "This grid yields 16 unique configurations (2 * 2 * 2 * 2), each to be trained and evaluated to compare loss, accuracy, and feature map evolution.\n",
        "\n",
        "The goal is to identify the optimal combinations for model performance."
      ],
      "metadata": {
        "id": "n3FM_T0m825w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Flattened Size Function\n",
        "\n",
        "Function to compute the current input size for the model."
      ],
      "metadata": {
        "id": "cQ60ADn__AIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_flattened_size(model, input_shape = (1, 28, 28)):\n",
        "  with torch.no_grad():\n",
        "    dummy = torch.zeros(1, *input_shape)\n",
        "    output = model(dummy)\n",
        "    return output.view(1, -1).shape[1]"
      ],
      "metadata": {
        "id": "z2blLsOP_JXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model Builder\n",
        "\n",
        "Implement modular construction of a CNN with the specified kernel size and stride."
      ],
      "metadata": {
        "id": "yf1Ei4_p91rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(kernel_size, stride):\n",
        "  feature_extractor = nn.Sequential(\n",
        "      nn.Conv2d(1, 6, kernel_size = kernel_size, stride = stride),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(6, 16, kernel_size = kernel_size, stride = stride),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten()\n",
        "  )\n",
        "\n",
        "  flattened_size = compute_flattened_size(feature_extractor)\n",
        "\n",
        "  return nn.Sequential(\n",
        "      feature_extractor,\n",
        "      nn.Linear(flattened_size, 10),\n",
        "      nn.Softmax(dim = 1)\n",
        "  )"
      ],
      "metadata": {
        "id": "T6ZNzUvi-JO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create DataLoader Factory\n",
        "\n",
        "Create data loaders with consistent batching to keep data handling clean and configurable."
      ],
      "metadata": {
        "id": "B5Ro0TWj_mTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(training_data, validation_data, batch_size):\n",
        "  training_loader = DataLoader(training_data, batch_size = batch_size, shuffle = True)\n",
        "  validation_loader = DataLoader(validation_data, batch_size = batch_size, shuffle = False)\n",
        "  testing_loader = DataLoader(testing_data, batch_size = batch_size, shuffle = False)\n",
        "  return training_loader, validation_loader, testing_loader"
      ],
      "metadata": {
        "id": "5wvWTGdD_wEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Train Model Function\n",
        "\n",
        "Function to train the model and return final loss and validation accuracy; modular design for reuse across sweeps."
      ],
      "metadata": {
        "id": "U25o_huUARuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_loader, validation_loader, lr, device = \"cpu\", epochs = 5):\n",
        "  model.to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for images, labels in training_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(images)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  correct, total = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in validation_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      output = model(images)\n",
        "      pred = output.argmax(dim = 1)\n",
        "      correct += (pred == labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "\n",
        "  val_accuracy = 100 * correct / total\n",
        "  return loss.item(), val_accuracy"
      ],
      "metadata": {
        "id": "UgIk0krYAdwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Sweep Runner Function\n",
        "\n",
        "Function to execute the full sweep across all parameter combinations.  Log results for analysis."
      ],
      "metadata": {
        "id": "SvQ52o-MB4kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sweep(training_data, validation_data, sweep_grid):\n",
        "  results = []\n",
        "\n",
        "  for k in sweep_grid[\"kernel_size\"]:\n",
        "    for s in sweep_grid[\"stride\"]:\n",
        "      for b in sweep_grid[\"batch_size\"]:\n",
        "        for lr in sweep_grid[\"learning_rate\"]:\n",
        "          print(f\"Running: kernel_size = {k}, stride = {s}, batch_size = {b}, learning_rate = {lr}\")\n",
        "          model = build_model(k, s)\n",
        "          training_loader, validation_loader = create_dataloaders(training_data, validation_data, b)\n",
        "          final_loss, val_acc = train_model(model, training_loader, validation_loader, lr)\n",
        "          results.append({\n",
        "              \"kernel_size\": k,\n",
        "              \"stride\": s,\n",
        "              \"batch_size\": b,\n",
        "              \"learning_rate\": lr,\n",
        "              \"final_loss\": round(final_loss, 4),\n",
        "              \"val_acc\": round(val_acc, 2)\n",
        "          })\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "-rRGWIxnCEGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Preview Results Function\n",
        "\n",
        "Display top-performing configurations for quick insight and stakeholder reporting."
      ],
      "metadata": {
        "id": "s5r_bW5gDZSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preview_results(results, top_n = 10):\n",
        "  df = pd.DataFrame(results)\n",
        "  print(df.sort_values(by = \"val_acc\", ascending = False).head(top_n))"
      ],
      "metadata": {
        "id": "pWjwZDjTDivQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}